IDOL(
  (detr): CondInst_segm(
    (detr): DeformableDETR(
      (transformer): DeformableTransformer(
        (encoder): DeformableTransformerEncoder(
          (layers): ModuleList(
            (0): DeformableTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.1, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.1, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (1): DeformableTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.1, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.1, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (2): DeformableTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.1, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.1, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (3): DeformableTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.1, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.1, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (4): DeformableTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.1, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.1, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (5): DeformableTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.1, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.1, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (decoder): DeformableTransformerDecoder(
          (layers): ModuleList(
            (0): DeformableTransformerDecoderLayer(
              (cross_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.1, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (dropout2): Dropout(p=0.1, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout3): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout4): Dropout(p=0.1, inplace=False)
              (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (1): DeformableTransformerDecoderLayer(
              (cross_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.1, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (dropout2): Dropout(p=0.1, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout3): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout4): Dropout(p=0.1, inplace=False)
              (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (2): DeformableTransformerDecoderLayer(
              (cross_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.1, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (dropout2): Dropout(p=0.1, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout3): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout4): Dropout(p=0.1, inplace=False)
              (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (3): DeformableTransformerDecoderLayer(
              (cross_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.1, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (dropout2): Dropout(p=0.1, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout3): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout4): Dropout(p=0.1, inplace=False)
              (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (4): DeformableTransformerDecoderLayer(
              (cross_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.1, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (dropout2): Dropout(p=0.1, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout3): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout4): Dropout(p=0.1, inplace=False)
              (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (5): DeformableTransformerDecoderLayer(
              (cross_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.1, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (dropout2): Dropout(p=0.1, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout3): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout4): Dropout(p=0.1, inplace=False)
              (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (bbox_embed): ModuleList(
            (0): MLP(
              (layers): ModuleList(
                (0): Linear(in_features=256, out_features=256, bias=True)
                (1): Linear(in_features=256, out_features=256, bias=True)
                (2): Linear(in_features=256, out_features=4, bias=True)
              )
            )
            (1): MLP(
              (layers): ModuleList(
                (0): Linear(in_features=256, out_features=256, bias=True)
                (1): Linear(in_features=256, out_features=256, bias=True)
                (2): Linear(in_features=256, out_features=4, bias=True)
              )
            )
            (2): MLP(
              (layers): ModuleList(
                (0): Linear(in_features=256, out_features=256, bias=True)
                (1): Linear(in_features=256, out_features=256, bias=True)
                (2): Linear(in_features=256, out_features=4, bias=True)
              )
            )
            (3): MLP(
              (layers): ModuleList(
                (0): Linear(in_features=256, out_features=256, bias=True)
                (1): Linear(in_features=256, out_features=256, bias=True)
                (2): Linear(in_features=256, out_features=4, bias=True)
              )
            )
            (4): MLP(
              (layers): ModuleList(
                (0): Linear(in_features=256, out_features=256, bias=True)
                (1): Linear(in_features=256, out_features=256, bias=True)
                (2): Linear(in_features=256, out_features=4, bias=True)
              )
            )
            (5): MLP(
              (layers): ModuleList(
                (0): Linear(in_features=256, out_features=256, bias=True)
                (1): Linear(in_features=256, out_features=256, bias=True)
                (2): Linear(in_features=256, out_features=4, bias=True)
              )
            )
          )
        )
        (reference_points): Linear(in_features=256, out_features=2, bias=True)
      )
      (class_embed): ModuleList(
        (0): Linear(in_features=256, out_features=80, bias=True)
        (1): Linear(in_features=256, out_features=80, bias=True)
        (2): Linear(in_features=256, out_features=80, bias=True)
        (3): Linear(in_features=256, out_features=80, bias=True)
        (4): Linear(in_features=256, out_features=80, bias=True)
        (5): Linear(in_features=256, out_features=80, bias=True)
      )
      (bbox_embed): ModuleList(
        (0): MLP(
          (layers): ModuleList(
            (0): Linear(in_features=256, out_features=256, bias=True)
            (1): Linear(in_features=256, out_features=256, bias=True)
            (2): Linear(in_features=256, out_features=4, bias=True)
          )
        )
        (1): MLP(
          (layers): ModuleList(
            (0): Linear(in_features=256, out_features=256, bias=True)
            (1): Linear(in_features=256, out_features=256, bias=True)
            (2): Linear(in_features=256, out_features=4, bias=True)
          )
        )
        (2): MLP(
          (layers): ModuleList(
            (0): Linear(in_features=256, out_features=256, bias=True)
            (1): Linear(in_features=256, out_features=256, bias=True)
            (2): Linear(in_features=256, out_features=4, bias=True)
          )
        )
        (3): MLP(
          (layers): ModuleList(
            (0): Linear(in_features=256, out_features=256, bias=True)
            (1): Linear(in_features=256, out_features=256, bias=True)
            (2): Linear(in_features=256, out_features=4, bias=True)
          )
        )
        (4): MLP(
          (layers): ModuleList(
            (0): Linear(in_features=256, out_features=256, bias=True)
            (1): Linear(in_features=256, out_features=256, bias=True)
            (2): Linear(in_features=256, out_features=4, bias=True)
          )
        )
        (5): MLP(
          (layers): ModuleList(
            (0): Linear(in_features=256, out_features=256, bias=True)
            (1): Linear(in_features=256, out_features=256, bias=True)
            (2): Linear(in_features=256, out_features=4, bias=True)
          )
        )
      )
      (query_embed): Embedding(300, 512)
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (3): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (backbone): Joiner(
        (0): MaskedBackbone(
          (backbone): ResNet(
            (stem): BasicStem(
              (conv1): Conv2d(
                3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
            )
            (res2): Sequential(
              (0): BottleneckBlock(
                (shortcut): Conv2d(
                  64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv1): Conv2d(
                  64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv2): Conv2d(
                  64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv3): Conv2d(
                  64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
              )
              (1): BottleneckBlock(
                (conv1): Conv2d(
                  256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv2): Conv2d(
                  64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv3): Conv2d(
                  64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
              )
              (2): BottleneckBlock(
                (conv1): Conv2d(
                  256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv2): Conv2d(
                  64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv3): Conv2d(
                  64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
              )
            )
            (res3): Sequential(
              (0): BottleneckBlock(
                (shortcut): Conv2d(
                  256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv1): Conv2d(
                  256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv2): Conv2d(
                  128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv3): Conv2d(
                  128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
              )
              (1): BottleneckBlock(
                (conv1): Conv2d(
                  512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv2): Conv2d(
                  128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv3): Conv2d(
                  128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
              )
              (2): BottleneckBlock(
                (conv1): Conv2d(
                  512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv2): Conv2d(
                  128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv3): Conv2d(
                  128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
              )
              (3): BottleneckBlock(
                (conv1): Conv2d(
                  512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv2): Conv2d(
                  128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv3): Conv2d(
                  128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
              )
            )
            (res4): Sequential(
              (0): BottleneckBlock(
                (shortcut): Conv2d(
                  512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
                (conv1): Conv2d(
                  512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
              (1): BottleneckBlock(
                (conv1): Conv2d(
                  1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
              (2): BottleneckBlock(
                (conv1): Conv2d(
                  1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
              (3): BottleneckBlock(
                (conv1): Conv2d(
                  1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
              (4): BottleneckBlock(
                (conv1): Conv2d(
                  1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
              (5): BottleneckBlock(
                (conv1): Conv2d(
                  1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
            )
            (res5): Sequential(
              (0): BottleneckBlock(
                (shortcut): Conv2d(
                  1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
                  (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
                )
                (conv1): Conv2d(
                  1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv2): Conv2d(
                  512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv3): Conv2d(
                  512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
                )
              )
              (1): BottleneckBlock(
                (conv1): Conv2d(
                  2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv2): Conv2d(
                  512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv3): Conv2d(
                  512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
                )
              )
              (2): BottleneckBlock(
                (conv1): Conv2d(
                  2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv2): Conv2d(
                  512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv3): Conv2d(
                  512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
                )
              )
            )
          )
        )
        (1): PositionEmbeddingSine()
      )
    )
    (controller): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=169, bias=True)
      )
    )
    (mask_head): MaskHeadSmallConv(
      (lay1): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (lay2): Conv2d(64, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (lay3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (lay4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (dcn): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (reid_embed_head): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=256, bias=True)
      )
    )
  )
  (criterion): SetCriterion(
    (matcher): HungarianMatcher()
  )
)